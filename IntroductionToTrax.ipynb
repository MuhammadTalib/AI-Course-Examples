{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntroductionToTrax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4AstWUNMhUVkTLoSyjQDD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadTalib/AI-Course-Examples/blob/master/IntroductionToTrax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNS4P3kOVMCi"
      },
      "source": [
        "### Imports"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-q2GHEyV1t4",
        "outputId": "6fc77f23-c27b-4879-9534-8a835bd7d271"
      },
      "source": [
        "!pip install trax==1.3.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting trax==1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/d8/ad90a5c79804561bbbc5fd65a4cb6b6e735370225e777cfc46980a9dc479/trax-1.3.1-py2.py3-none-any.whl (347kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 5.6MB/s \n",
            "\u001b[?25hCollecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c6/2ea21c983ae27553a798829a533349de5df99678cfd3fd8d313ae30b063f/t5-0.8.1-py3-none-any.whl (214kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 41.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (1.4.1)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (1.15.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.1.59+cuda101)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.2.8)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (4.0.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.17.3)\n",
            "Collecting tensor2tensor\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/7c/9e87d30cefad5cbc390bb7f626efb3ded9b19416b8160f1a1278da81b218/tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 47.4MB/s \n",
            "\u001b[?25hCollecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/86/22ad798f94d564c3e423758b60ddd3689e83ad629b3f31ff2ae45a6e3eed/tensorflow_text-2.4.3-cp36-cp36m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 34.9MB/s \n",
            "\u001b[?25hCollecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/20/23bbc94034e16bb1ace73e9e7922226e31d6d36b88dcfa257d2c59b3f465/mesh_tensorflow-0.1.18-py3-none-any.whl (361kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 40.1MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (2.9.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (0.22.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (3.2.5)\n",
            "Collecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 42.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 38.2MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/85/8dd62567f9840b314f3610bd1026aa6326fca96c2e3a0c47a033bbde36de/tfds_nightly-4.2.0.dev202101300107-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 22.0MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (1.7.0+cu101)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (1.1.5)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.6/dist-packages (from jaxlib->trax==1.3.1) (1.12)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->trax==1.3.1) (3.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (20.3.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (0.1.5)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (0.3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (3.12.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (0.8)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (5.1.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (0.27.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (2.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax==1.3.1) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax==1.3.1) (1.3.0)\n",
            "Collecting kfac\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/36/06fe2c757044bb51906fef231ac48cc5bf9a277fc9a8c7e1108d7e9e8cfd/kfac-0.2.3-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (0.8.3)\n",
            "Collecting tensorflow-probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (4.1.3)\n",
            "Collecting bz2file\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Collecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 30.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.1.2)\n",
            "Collecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (2.10.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.7.12)\n",
            "Collecting tf-slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (7.0.0)\n",
            "Requirement already satisfied: dopamine-rl in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.0.5)\n",
            "Collecting tensorflow-gan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 26.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (4.1.2.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.1.1)\n",
            "Collecting gevent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/90/000736e587a720f8eef2bcd384456ce2add5ddfc3c63cf51a7ea13412cb6/gevent-21.1.2-cp36-cp36m-manylinux2010_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 17.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax==1.3.1) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax==1.3.1) (0.11.0)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/14/47/43fdee732cda080367c7d2a1ee5aee3b31e49baf629029a386df458d2dd3/portalocker-2.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel->t5->trax==1.3.1) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5->trax==1.3.1) (1.0.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax==1.3.1) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax==1.3.1) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 48.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax==1.3.1) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax==1.3.1) (20.8)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5->trax==1.3.1) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax==1.3.1) (2.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.3.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.3.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.3.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.3.1) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax==1.3.1) (51.3.3)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax==1.3.1) (3.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax==1.3.1) (1.52.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tensor2tensor->trax==1.3.1) (2.7.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor->trax==1.3.1) (4.4.2)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax==1.3.1) (0.17.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax==1.3.1) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax==1.3.1) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax==1.3.1) (4.7)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (2.11.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (0.0.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy->tensor2tensor->trax==1.3.1) (1.1.0)\n",
            "Collecting greenlet<2.0,>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/e2/9fbb24cf1ee89813ded3761314562a83a2822ad2bf5682eef0d0c99e2a5d/greenlet-1.0.0-cp36-cp36m-manylinux2010_x86_64.whl (156kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 36.8MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (2.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (1.32.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (2.4.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (0.3.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5->trax==1.3.1) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->tensor2tensor->trax==1.3.1) (1.1.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client->tensor2tensor->trax==1.3.1) (4.2.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (3.3.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->trax==1.3.1) (3.1.0)\n",
            "Building wheels for collected packages: bz2file, pypng, sacremoses\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-cp36-none-any.whl size=6883 sha256=1c6e057ab45c413d8b59336e320efaa4d41a3d70262cf51f91062e0602a0c8c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp36-none-any.whl size=67161 sha256=24ee6c15c2c8996fb1e2b3ffb7163ed19b8f35f304e12b09bfa5fa09204d40b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=806773422a3fee702fd6101574215d8217191a65ba7da850220f5b8766f5fc98\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built bz2file pypng sacremoses\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: mesh-tensorflow, portalocker, sacrebleu, sacremoses, tokenizers, transformers, sentencepiece, tfds-nightly, rouge-score, tensorflow-text, t5, tensorflow-probability, kfac, bz2file, pypng, gunicorn, tf-slim, tensorflow-gan, greenlet, zope.event, zope.interface, gevent, tensor2tensor, funcsigs, trax\n",
            "  Found existing installation: tensorflow-probability 0.12.1\n",
            "    Uninstalling tensorflow-probability-0.12.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.12.1\n",
            "Successfully installed bz2file-0.98 funcsigs-1.0.2 gevent-21.1.2 greenlet-1.0.0 gunicorn-20.0.4 kfac-0.2.3 mesh-tensorflow-0.1.18 portalocker-2.2.0 pypng-0.0.20 rouge-score-0.0.4 sacrebleu-1.5.0 sacremoses-0.0.43 sentencepiece-0.1.95 t5-0.8.1 tensor2tensor-1.15.7 tensorflow-gan-2.0.0 tensorflow-probability-0.7.0 tensorflow-text-2.4.3 tf-slim-1.1.0 tfds-nightly-4.2.0.dev202101300107 tokenizers-0.9.4 transformers-4.2.2 trax-1.3.1 zope.event-4.5.0 zope.interface-5.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtXBm4uEVq_9",
        "outputId": "78b521c5-b6a2-432e-c0b8-70916d517132"
      },
      "source": [
        "import numpy as np  # regular ol' numpy\r\n",
        "\r\n",
        "from trax import layers as tl  # core building block\r\n",
        "from trax import shapes  # data signatures: dimensionality and type\r\n",
        "from trax import fastmath  # uses jax, offers numpy on steroids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
            "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2S99YcLV-fb"
      },
      "source": [
        "#pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lktEnCdlWE3A",
        "outputId": "bf4f30b8-e809-4512-cdec-30b8cc315778"
      },
      "source": [
        "# Layers\r\n",
        "# Create a relu trax layer\r\n",
        "relu = tl.Relu()\r\n",
        "\r\n",
        "# Inspect properties\r\n",
        "print(\"-- Properties --\")\r\n",
        "print(\"name :\", relu.name)\r\n",
        "print(\"expected inputs :\", relu.n_in)\r\n",
        "print(\"promised outputs :\", relu.n_out, \"\\n\")\r\n",
        "\r\n",
        "# Inputs\r\n",
        "x = np.array([-2, -1, 0, 1, 2])\r\n",
        "print(\"-- Inputs --\")\r\n",
        "print(\"x :\", x, \"\\n\")\r\n",
        "\r\n",
        "# Outputs\r\n",
        "y = relu(x)\r\n",
        "print(\"-- Outputs --\")\r\n",
        "print(\"y :\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Properties --\n",
            "name : Relu\n",
            "expected inputs : 1\n",
            "promised outputs : 1 \n",
            "\n",
            "-- Inputs --\n",
            "x : [-2 -1  0  1  2] \n",
            "\n",
            "-- Outputs --\n",
            "y : [0 0 0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDjaJukIWJzb",
        "outputId": "be2bca0c-8fe7-4864-eca3-bf48cf85a0b7"
      },
      "source": [
        "# Create a concatenate trax layer\r\n",
        "concat = tl.Concatenate()\r\n",
        "print(\"-- Properties --\")\r\n",
        "print(\"name :\", concat.name)\r\n",
        "print(\"expected inputs :\", concat.n_in)\r\n",
        "print(\"promised outputs :\", concat.n_out, \"\\n\")\r\n",
        "\r\n",
        "# Inputs\r\n",
        "x1 = np.array([-10, -20, -30])\r\n",
        "x2 = x1 / -10\r\n",
        "print(\"-- Inputs --\")\r\n",
        "print(\"x1 :\", x1)\r\n",
        "print(\"x2 :\", x2, \"\\n\")\r\n",
        "\r\n",
        "# Outputs\r\n",
        "y = concat([x1, x2])\r\n",
        "print(\"-- Outputs --\")\r\n",
        "print(\"y :\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Properties --\n",
            "name : Concatenate\n",
            "expected inputs : 2\n",
            "promised outputs : 1 \n",
            "\n",
            "-- Inputs --\n",
            "x1 : [-10 -20 -30]\n",
            "x2 : [1. 2. 3.] \n",
            "\n",
            "-- Outputs --\n",
            "y : [-10. -20. -30.   1.   2.   3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MSUS5e0WWUG"
      },
      "source": [
        "## Layers are Configurable\r\n",
        "#You can change the default settings of layers. For example, you can change the expected inputs for a concatenate layer from 2 to 3 using the optional parameter n_items."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6X7m05iWxpB",
        "outputId": "3775d479-afad-41f9-9453-8f3fcf596616"
      },
      "source": [
        "# Configure a concatenate layer\r\n",
        "concat_3 = tl.Concatenate(n_items=3)  # configure the layer's expected inputs\r\n",
        "print(\"-- Properties --\")\r\n",
        "print(\"name :\", concat_3.name)\r\n",
        "print(\"expected inputs :\", concat_3.n_in)\r\n",
        "print(\"promised outputs :\", concat_3.n_out, \"\\n\")\r\n",
        "\r\n",
        "# Inputs\r\n",
        "x1 = np.array([-10, -20, -30])\r\n",
        "x2 = x1 / -10\r\n",
        "x3 = x2 * 0.99\r\n",
        "print(\"-- Inputs --\")\r\n",
        "print(\"x1 :\", x1)\r\n",
        "print(\"x2 :\", x2)\r\n",
        "print(\"x3 :\", x3, \"\\n\")\r\n",
        "\r\n",
        "# Outputs\r\n",
        "y = concat_3([x1, x2, x3])\r\n",
        "print(\"-- Outputs --\")\r\n",
        "print(\"y :\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Properties --\n",
            "name : Concatenate\n",
            "expected inputs : 3\n",
            "promised outputs : 1 \n",
            "\n",
            "-- Inputs --\n",
            "x1 : [-10 -20 -30]\n",
            "x2 : [1. 2. 3.]\n",
            "x3 : [0.99 1.98 2.97] \n",
            "\n",
            "-- Outputs --\n",
            "y : [-10.   -20.   -30.     1.     2.     3.     0.99   1.98   2.97]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBdbSUzmW9EX",
        "outputId": "b614e183-62d9-4e99-8362-2798ae4cb3a1"
      },
      "source": [
        "#Note: At any point,if you want to refer the function help/ look up the documentation or use help function.\r\n",
        "help(tl.Concatenate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Concatenate in module trax.layers.combinators:\n",
            "\n",
            "class Concatenate(trax.layers.base.Layer)\n",
            " |  Concatenates n tensors into a single tensor.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Concatenate\n",
            " |      trax.layers.base.Layer\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, n_items=2, axis=-1)\n",
            " |      Creates a partially initialized, unconnected layer instance.\n",
            " |      \n",
            " |      Args:\n",
            " |        n_in: Number of inputs expected by this layer.\n",
            " |        n_out: Number of outputs promised by this layer.\n",
            " |        name: Class-like name for this layer; for use when printing this layer.\n",
            " |        sublayers_to_print: Sublayers to display when printing out this layer;\n",
            " |          By default (when None) we display all sublayers.\n",
            " |  \n",
            " |  forward(self, xs)\n",
            " |      Computes this layer's output as part of a forward pass through the model.\n",
            " |      \n",
            " |      Authors of new layer subclasses should override this method to define the\n",
            " |      forward computation that their layer performs. Use `self.weights` to access\n",
            " |      trainable weights of this layer. If you need to use local non-trainable\n",
            " |      state or randomness, use `self.rng` for the random seed (no need to set it)\n",
            " |      and use `self.state` for non-trainable state (and set it to the new value).\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Zero or more input tensors, packaged as described in the `Layer`\n",
            " |            class docstring.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Zero or more output tensors, packaged as described in the `Layer` class\n",
            " |        docstring.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from trax.layers.base.Layer:\n",
            " |  \n",
            " |  __call__(self, x, weights=None, state=None, rng=None)\n",
            " |      Makes layers callable; for use in tests or interactive settings.\n",
            " |      \n",
            " |      This convenience method helps library users play with, test, or otherwise\n",
            " |      probe the behavior of layers outside of a full training environment. It\n",
            " |      presents the layer as callable function from inputs to outputs, with the\n",
            " |      option of manually specifying weights and non-parameter state per individual\n",
            " |      call. For convenience, weights and non-parameter state are cached per layer\n",
            " |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n",
            " |      and acquiring non-empty values either by initialization or from values\n",
            " |      explicitly provided via the weights and state keyword arguments.\n",
            " |      \n",
            " |      Args:\n",
            " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
            " |            docstring.\n",
            " |        weights: Weights or `None`; if `None`, use self's cached weights value.\n",
            " |        state: State or `None`; if `None`, use self's cached state value.\n",
            " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
            " |            if `None`, use a default computed from an integer 0 seed.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Zero or more output tensors, packaged as described in the `Layer` class\n",
            " |        docstring.\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n",
            " |      Custom backward pass to propagate gradients in a custom way.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensors; can be a (possibly nested) tuple.\n",
            " |        output: The result of running this layer on inputs.\n",
            " |        grad: Gradient signal computed based on subsequent layers; its structure\n",
            " |            and shape must match output.\n",
            " |        weights: This layer's weights.\n",
            " |        state: This layer's state prior to the current forward pass.\n",
            " |        new_state: This layer's state after the current forward pass.\n",
            " |        rng: Single-use random number generator (JAX PRNG key).\n",
            " |      \n",
            " |      Returns:\n",
            " |        The custom gradient signal for the input. Note that we need to return\n",
            " |        a gradient for each argument of forward, so it will usually be a tuple\n",
            " |        of signals: the gradient for inputs and weights.\n",
            " |  \n",
            " |  init(self, input_signature, rng=None, use_cache=False)\n",
            " |      Initializes weights/state of this layer and its sublayers recursively.\n",
            " |      \n",
            " |      Initialization creates layer weights and state, for layers that use them.\n",
            " |      It derives the necessary array shapes and data types from the layer's input\n",
            " |      signature, which is itself just shape and data type information.\n",
            " |      \n",
            " |      For layers without weights or state, this method safely does nothing.\n",
            " |      \n",
            " |      This method is designed to create weights/state only once for each layer\n",
            " |      instance, even if the same layer instance occurs in multiple places in the\n",
            " |      network. This enables weight sharing to be implemented as layer sharing.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n",
            " |            or list/tuple of `ShapeDtype` instances.\n",
            " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
            " |            if `None`, use a default computed from an integer 0 seed.\n",
            " |        use_cache: If `True`, and if this layer instance has already been\n",
            " |            initialized elsewhere in the network, then return special marker\n",
            " |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n",
            " |            Else return this layer's newly initialized weights and state.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `(weights, state)` tuple.\n",
            " |  \n",
            " |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n",
            " |      Initializes this layer and its sublayers from a pickled checkpoint.\n",
            " |      \n",
            " |      In the common case (`weights_only=False`), the file must be a gziped pickled\n",
            " |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n",
            " |      `'input_signature'`, which are used to initialize this layer.\n",
            " |      If `input_signature` is specified, it's used instead of the one in the file.\n",
            " |      If `weights_only` is `True`, the dictionary does not need to have the\n",
            " |      `'flat_state'` item and the state it not restored either.\n",
            " |      \n",
            " |      Args:\n",
            " |        file_name: Name/path of the pickeled weights/state file.\n",
            " |        weights_only: If `True`, initialize only the layer's weights. Else\n",
            " |            initialize both weights and state.\n",
            " |        input_signature: Input signature to be used instead of the one from file.\n",
            " |  \n",
            " |  init_weights_and_state(self, input_signature)\n",
            " |      Initializes weights and state for inputs with the given signature.\n",
            " |      \n",
            " |      Authors of new layer subclasses should override this method if their layer\n",
            " |      uses trainable weights or non-trainable state. To initialize trainable\n",
            " |      weights, set `self.weights` and to initialize non-trainable state,\n",
            " |      set `self.state` to the intended value.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: A `ShapeDtype` instance (if this layer takes one input)\n",
            " |            or a list/tuple of `ShapeDtype` instances; signatures of inputs.\n",
            " |  \n",
            " |  output_signature(self, input_signature)\n",
            " |      Returns output signature this layer would give for `input_signature`.\n",
            " |  \n",
            " |  pure_fn(self, x, weights, state, rng, use_cache=False)\n",
            " |      Applies this layer as a pure function with no optional args.\n",
            " |      \n",
            " |      This method exposes the layer's computation as a pure function. This is\n",
            " |      especially useful for JIT compilation. Do not override, use `forward`\n",
            " |      instead.\n",
            " |      \n",
            " |      Args:\n",
            " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
            " |            docstring.\n",
            " |        weights: A tuple or list of trainable weights, with one element for this\n",
            " |            layer if this layer has no sublayers, or one for each sublayer if\n",
            " |            this layer has sublayers. If a layer (or sublayer) has no trainable\n",
            " |            weights, the corresponding weights element is an empty tuple.\n",
            " |        state: Layer-specific non-parameter state that can update between batches.\n",
            " |        rng: Single-use random number generator (JAX PRNG key).\n",
            " |        use_cache: if `True`, cache weights and state in the layer object; used\n",
            " |          to implement layer sharing in combinators.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n",
            " |        promised by this layer, and are packaged as described in the `Layer`\n",
            " |        class docstring.\n",
            " |  \n",
            " |  weights_and_state_signature(self, input_signature)\n",
            " |      Return a pair containing the signatures of weights and state.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from trax.layers.base.Layer:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  has_backward\n",
            " |      Returns `True` if this layer provides its own custom backward pass code.\n",
            " |      \n",
            " |      A layer subclass that provides custom backward pass code (for custom\n",
            " |      gradients) must override this method to return `True`.\n",
            " |  \n",
            " |  n_in\n",
            " |      Returns how many tensors this layer expects as input.\n",
            " |  \n",
            " |  n_out\n",
            " |      Returns how many tensors this layer promises as output.\n",
            " |  \n",
            " |  name\n",
            " |      Returns the name of this layer.\n",
            " |  \n",
            " |  rng\n",
            " |      Returns a single-use random number generator without advancing it.\n",
            " |  \n",
            " |  state\n",
            " |      Returns a tuple containing this layer's state; may be empty.\n",
            " |  \n",
            " |  sublayers\n",
            " |      Returns a tuple containing this layer's sublayers; may be empty.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns this layer's weights.\n",
            " |      \n",
            " |      Depending on the layer, the weights can be in the form of:\n",
            " |      \n",
            " |        - an empty tuple\n",
            " |        - a tensor (ndarray)\n",
            " |        - a nested structure of tuples and tensors\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q2nRDlpXfrY"
      },
      "source": [
        "Layers can have Weights\r\n",
        "Some layer types include mutable weights and biases that are used in computation and training. Layers of this type require initialization before use.\r\n",
        "\r\n",
        "For example the LayerNorm layer calculates normalized data, that is also scaled by weights and biases. During initialization you pass the data shape and data type of the inputs, so the layer can initialize compatible arrays of weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp4kgIzRYP7H",
        "outputId": "488d9268-3c7b-4bdb-aa3a-6fc7c597151e"
      },
      "source": [
        "# Uncomment any of them to see information regarding the function\r\n",
        "help(tl.LayerNorm)\r\n",
        "help(shapes.signature)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class LayerNorm in module trax.layers.normalization:\n",
            "\n",
            "class LayerNorm(trax.layers.base.Layer)\n",
            " |  Layer normalization.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      LayerNorm\n",
            " |      trax.layers.base.Layer\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, epsilon=1e-06)\n",
            " |      Creates a partially initialized, unconnected layer instance.\n",
            " |      \n",
            " |      Args:\n",
            " |        n_in: Number of inputs expected by this layer.\n",
            " |        n_out: Number of outputs promised by this layer.\n",
            " |        name: Class-like name for this layer; for use when printing this layer.\n",
            " |        sublayers_to_print: Sublayers to display when printing out this layer;\n",
            " |          By default (when None) we display all sublayers.\n",
            " |  \n",
            " |  forward(self, x)\n",
            " |      Computes this layer's output as part of a forward pass through the model.\n",
            " |      \n",
            " |      Authors of new layer subclasses should override this method to define the\n",
            " |      forward computation that their layer performs. Use `self.weights` to access\n",
            " |      trainable weights of this layer. If you need to use local non-trainable\n",
            " |      state or randomness, use `self.rng` for the random seed (no need to set it)\n",
            " |      and use `self.state` for non-trainable state (and set it to the new value).\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Zero or more input tensors, packaged as described in the `Layer`\n",
            " |            class docstring.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Zero or more output tensors, packaged as described in the `Layer` class\n",
            " |        docstring.\n",
            " |  \n",
            " |  init_weights_and_state(self, input_signature)\n",
            " |      Initializes weights and state for inputs with the given signature.\n",
            " |      \n",
            " |      Authors of new layer subclasses should override this method if their layer\n",
            " |      uses trainable weights or non-trainable state. To initialize trainable\n",
            " |      weights, set `self.weights` and to initialize non-trainable state,\n",
            " |      set `self.state` to the intended value.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: A `ShapeDtype` instance (if this layer takes one input)\n",
            " |            or a list/tuple of `ShapeDtype` instances; signatures of inputs.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from trax.layers.base.Layer:\n",
            " |  \n",
            " |  __call__(self, x, weights=None, state=None, rng=None)\n",
            " |      Makes layers callable; for use in tests or interactive settings.\n",
            " |      \n",
            " |      This convenience method helps library users play with, test, or otherwise\n",
            " |      probe the behavior of layers outside of a full training environment. It\n",
            " |      presents the layer as callable function from inputs to outputs, with the\n",
            " |      option of manually specifying weights and non-parameter state per individual\n",
            " |      call. For convenience, weights and non-parameter state are cached per layer\n",
            " |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n",
            " |      and acquiring non-empty values either by initialization or from values\n",
            " |      explicitly provided via the weights and state keyword arguments.\n",
            " |      \n",
            " |      Args:\n",
            " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
            " |            docstring.\n",
            " |        weights: Weights or `None`; if `None`, use self's cached weights value.\n",
            " |        state: State or `None`; if `None`, use self's cached state value.\n",
            " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
            " |            if `None`, use a default computed from an integer 0 seed.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Zero or more output tensors, packaged as described in the `Layer` class\n",
            " |        docstring.\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n",
            " |      Custom backward pass to propagate gradients in a custom way.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensors; can be a (possibly nested) tuple.\n",
            " |        output: The result of running this layer on inputs.\n",
            " |        grad: Gradient signal computed based on subsequent layers; its structure\n",
            " |            and shape must match output.\n",
            " |        weights: This layer's weights.\n",
            " |        state: This layer's state prior to the current forward pass.\n",
            " |        new_state: This layer's state after the current forward pass.\n",
            " |        rng: Single-use random number generator (JAX PRNG key).\n",
            " |      \n",
            " |      Returns:\n",
            " |        The custom gradient signal for the input. Note that we need to return\n",
            " |        a gradient for each argument of forward, so it will usually be a tuple\n",
            " |        of signals: the gradient for inputs and weights.\n",
            " |  \n",
            " |  init(self, input_signature, rng=None, use_cache=False)\n",
            " |      Initializes weights/state of this layer and its sublayers recursively.\n",
            " |      \n",
            " |      Initialization creates layer weights and state, for layers that use them.\n",
            " |      It derives the necessary array shapes and data types from the layer's input\n",
            " |      signature, which is itself just shape and data type information.\n",
            " |      \n",
            " |      For layers without weights or state, this method safely does nothing.\n",
            " |      \n",
            " |      This method is designed to create weights/state only once for each layer\n",
            " |      instance, even if the same layer instance occurs in multiple places in the\n",
            " |      network. This enables weight sharing to be implemented as layer sharing.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n",
            " |            or list/tuple of `ShapeDtype` instances.\n",
            " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
            " |            if `None`, use a default computed from an integer 0 seed.\n",
            " |        use_cache: If `True`, and if this layer instance has already been\n",
            " |            initialized elsewhere in the network, then return special marker\n",
            " |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n",
            " |            Else return this layer's newly initialized weights and state.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `(weights, state)` tuple.\n",
            " |  \n",
            " |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n",
            " |      Initializes this layer and its sublayers from a pickled checkpoint.\n",
            " |      \n",
            " |      In the common case (`weights_only=False`), the file must be a gziped pickled\n",
            " |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n",
            " |      `'input_signature'`, which are used to initialize this layer.\n",
            " |      If `input_signature` is specified, it's used instead of the one in the file.\n",
            " |      If `weights_only` is `True`, the dictionary does not need to have the\n",
            " |      `'flat_state'` item and the state it not restored either.\n",
            " |      \n",
            " |      Args:\n",
            " |        file_name: Name/path of the pickeled weights/state file.\n",
            " |        weights_only: If `True`, initialize only the layer's weights. Else\n",
            " |            initialize both weights and state.\n",
            " |        input_signature: Input signature to be used instead of the one from file.\n",
            " |  \n",
            " |  output_signature(self, input_signature)\n",
            " |      Returns output signature this layer would give for `input_signature`.\n",
            " |  \n",
            " |  pure_fn(self, x, weights, state, rng, use_cache=False)\n",
            " |      Applies this layer as a pure function with no optional args.\n",
            " |      \n",
            " |      This method exposes the layer's computation as a pure function. This is\n",
            " |      especially useful for JIT compilation. Do not override, use `forward`\n",
            " |      instead.\n",
            " |      \n",
            " |      Args:\n",
            " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
            " |            docstring.\n",
            " |        weights: A tuple or list of trainable weights, with one element for this\n",
            " |            layer if this layer has no sublayers, or one for each sublayer if\n",
            " |            this layer has sublayers. If a layer (or sublayer) has no trainable\n",
            " |            weights, the corresponding weights element is an empty tuple.\n",
            " |        state: Layer-specific non-parameter state that can update between batches.\n",
            " |        rng: Single-use random number generator (JAX PRNG key).\n",
            " |        use_cache: if `True`, cache weights and state in the layer object; used\n",
            " |          to implement layer sharing in combinators.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n",
            " |        promised by this layer, and are packaged as described in the `Layer`\n",
            " |        class docstring.\n",
            " |  \n",
            " |  weights_and_state_signature(self, input_signature)\n",
            " |      Return a pair containing the signatures of weights and state.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from trax.layers.base.Layer:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  has_backward\n",
            " |      Returns `True` if this layer provides its own custom backward pass code.\n",
            " |      \n",
            " |      A layer subclass that provides custom backward pass code (for custom\n",
            " |      gradients) must override this method to return `True`.\n",
            " |  \n",
            " |  n_in\n",
            " |      Returns how many tensors this layer expects as input.\n",
            " |  \n",
            " |  n_out\n",
            " |      Returns how many tensors this layer promises as output.\n",
            " |  \n",
            " |  name\n",
            " |      Returns the name of this layer.\n",
            " |  \n",
            " |  rng\n",
            " |      Returns a single-use random number generator without advancing it.\n",
            " |  \n",
            " |  state\n",
            " |      Returns a tuple containing this layer's state; may be empty.\n",
            " |  \n",
            " |  sublayers\n",
            " |      Returns a tuple containing this layer's sublayers; may be empty.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns this layer's weights.\n",
            " |      \n",
            " |      Depending on the layer, the weights can be in the form of:\n",
            " |      \n",
            " |        - an empty tuple\n",
            " |        - a tensor (ndarray)\n",
            " |        - a nested structure of tuples and tensors\n",
            "\n",
            "Help on function signature in module trax.shapes:\n",
            "\n",
            "signature(obj)\n",
            "    Returns a `ShapeDtype` signature for the given `obj`.\n",
            "    \n",
            "    A signature is either a `ShapeDtype` instance or a tuple of `ShapeDtype`\n",
            "    instances. Note that this function is permissive with respect to its inputs\n",
            "    (accepts lists or tuples or dicts, and underlying objects can be any type\n",
            "    as long as they have shape and dtype attributes) and returns the corresponding\n",
            "    nested structure of `ShapeDtype`.\n",
            "    \n",
            "    Args:\n",
            "      obj: An object that has `shape` and `dtype` attributes, or a list/tuple/dict\n",
            "          of such objects.\n",
            "    \n",
            "    Returns:\n",
            "      A corresponding nested structure of `ShapeDtype` instances.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh_P2gnqXkco",
        "outputId": "6d79fca0-a897-4dac-ba6c-079c87715e2a"
      },
      "source": [
        "# Layer initialization\r\n",
        "norm = tl.LayerNorm()\r\n",
        "# You first must know what the input data will look like\r\n",
        "x = np.array([0, 1, 2, 3], dtype=\"float\")\r\n",
        "\r\n",
        "# Use the input data signature to get shape and type for initializing weights and biases\r\n",
        "norm.init(shapes.signature(x)) # We need to convert the input datatype from usual tuple to trax ShapeDtype\r\n",
        "\r\n",
        "print(\"Normal shape:\",x.shape, \"Data Type:\",type(x.shape))\r\n",
        "print(\"Shapes Trax:\",shapes.signature(x),\"Data Type:\",type(shapes.signature(x)))\r\n",
        "\r\n",
        "# Inspect properties\r\n",
        "print(\"-- Properties --\")\r\n",
        "print(\"name :\", norm.name)\r\n",
        "print(\"expected inputs :\", norm.n_in)\r\n",
        "print(\"promised outputs :\", norm.n_out)\r\n",
        "# Weights and biases\r\n",
        "print(\"weights :\", norm.weights[0])\r\n",
        "print(\"biases :\", norm.weights[1], \"\\n\")\r\n",
        "# Inputs\r\n",
        "print(\"-- Inputs --\")\r\n",
        "print(\"x :\", x)\r\n",
        "\r\n",
        "# Outputs\r\n",
        "y = norm(x)\r\n",
        "print(\"-- Outputs --\")\r\n",
        "print(\"y :\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal shape: (4,) Data Type: <class 'tuple'>\n",
            "Shapes Trax: ShapeDtype{shape:(4,), dtype:float64} Data Type: <class 'trax.shapes.ShapeDtype'>\n",
            "-- Properties --\n",
            "name : LayerNorm\n",
            "expected inputs : 1\n",
            "promised outputs : 1\n",
            "weights : [1. 1. 1. 1.]\n",
            "biases : [0. 0. 0. 0.] \n",
            "\n",
            "-- Inputs --\n",
            "x : [0. 1. 2. 3.]\n",
            "-- Outputs --\n",
            "y : [-1.3416404  -0.44721344  0.44721344  1.3416404 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/_src/numpy/lax_numpy.py:2894: UserWarning: Explicitly requested dtype float64 requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"ones\")\n",
            "/usr/local/lib/python3.6/dist-packages/jax/_src/numpy/lax_numpy.py:2885: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKfUsdzuYO02"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u36blLE2XewG"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# **Custom Layers**\r\n",
        "This is where things start getting more interesting! You can create your own custom layers too and define custom functions for computations by using tl.Fn. Let me show you how.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcNFkJCiZRF6",
        "outputId": "0afc886e-7a43-4e47-d2cf-2e4aa1d14a52"
      },
      "source": [
        "help(tl.Fn)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function Fn in module trax.layers.base:\n",
            "\n",
            "Fn(name, f, n_out=1)\n",
            "    Returns a layer with no weights that applies the function `f`.\n",
            "    \n",
            "    `f` can take and return any number of arguments, and takes only positional\n",
            "    arguments -- no default or keyword arguments. It often uses JAX-numpy (`jnp`).\n",
            "    The following, for example, would create a layer that takes two inputs and\n",
            "    returns two outputs -- element-wise sums and maxima:\n",
            "    \n",
            "        `Fn('SumAndMax', lambda x0, x1: (x0 + x1, jnp.maximum(x0, x1)), n_out=2)`\n",
            "    \n",
            "    The layer's number of inputs (`n_in`) is automatically set to number of\n",
            "    positional arguments in `f`, but you must explicitly set the number of\n",
            "    outputs (`n_out`) whenever it's not the default value 1.\n",
            "    \n",
            "    Args:\n",
            "      name: Class-like name for the resulting layer; for use in debugging.\n",
            "      f: Pure function from input tensors to output tensors, where each input\n",
            "          tensor is a separate positional arg, e.g., `f(x0, x1) --> x0 + x1`.\n",
            "          Output tensors must be packaged as specified in the `Layer` class\n",
            "          docstring.\n",
            "      n_out: Number of outputs promised by the layer; default value 1.\n",
            "    \n",
            "    Returns:\n",
            "      Layer executing the function `f`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLSIPNWHZVUG",
        "outputId": "779537f1-c1b2-421b-c2d7-64ba3e155f79"
      },
      "source": [
        "# In this example you will create a layer to calculate the input times 2\r\n",
        "\r\n",
        "def TimesTwo():\r\n",
        "    layer_name = \"TimesTwo\" #don't forget to give your custom layer a name to identify\r\n",
        "\r\n",
        "    # Custom function for the custom layer\r\n",
        "    def func(x):\r\n",
        "        return x * 2\r\n",
        "\r\n",
        "    return tl.Fn(layer_name, func)\r\n",
        "\r\n",
        "\r\n",
        "# Test it\r\n",
        "times_two = TimesTwo()\r\n",
        "\r\n",
        "# Inspect properties\r\n",
        "print(\"-- Properties --\")\r\n",
        "print(\"name :\", times_two.name)\r\n",
        "print(\"expected inputs :\", times_two.n_in)\r\n",
        "print(\"promised outputs :\", times_two.n_out, \"\\n\")\r\n",
        "\r\n",
        "# Inputs\r\n",
        "x = np.array([1, 2, 3])\r\n",
        "print(\"-- Inputs --\")\r\n",
        "print(\"x :\", x, \"\\n\")\r\n",
        "\r\n",
        "# Outputs\r\n",
        "y = times_two(x)\r\n",
        "print(\"-- Outputs --\")\r\n",
        "print(\"y :\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Properties --\n",
            "name : TimesTwo\n",
            "expected inputs : 1\n",
            "promised outputs : 1 \n",
            "\n",
            "-- Inputs --\n",
            "x : [1 2 3] \n",
            "\n",
            "-- Outputs --\n",
            "y : [2 4 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAA8WQJIZbuZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoZ5Bpzjahml"
      },
      "source": [
        "\r\n",
        "# Combinators\r\n",
        "You can combine layers to build more complex layers. Trax provides a set of objects named combinator layers to make this happen. Combinators are themselves layers, so behavior commutes.\r\n",
        "\r\n",
        "# Serial Combinator\r\n",
        "This is the most common and easiest to use. For example could build a simple neural network by combining layers into a single layer using the Serial combinator. This new layer then acts just like a single layer, so you can inspect intputs, outputs and weights. Or even combine it into another layer! Combinators can then be used as trainable models. Try adding more layers\r\n",
        "\r\n",
        "##### **Note:As you must have guessed, if there is serial combinator, there must be a parallel combinator as well. Do try to explore about combinators and other layers from the trax documentation and look at the repo to understand how these layers are written.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-XKGGBPaxhC",
        "outputId": "29dda5a6-9819-4e00-e7a4-e0e2ba1a579e"
      },
      "source": [
        "serial = tl.Serial(\r\n",
        "    tl.LayerNorm(),         # normalize input\r\n",
        "    tl.Relu(),              # convert negative values to zero\r\n",
        "    times_two,              # the custom layer you created above, multiplies the input recieved from above by 2\r\n",
        "    \r\n",
        "    ### START CODE HERE\r\n",
        "#     tl.Dense(n_units=2),  # try adding more layers. eg uncomment these lines\r\n",
        "#     tl.Dense(n_units=1),  # Binary classification, maybe? uncomment at your own peril\r\n",
        "#     tl.LogSoftmax()       # Yes, LogSoftmax is also a layer\r\n",
        "    ### END CODE HERE\r\n",
        ")\r\n",
        "\r\n",
        "# Initialization\r\n",
        "x = np.array([-2, -1, 0, 1, 2]) #input\r\n",
        "serial.init(shapes.signature(x)) #initialising serial instance\r\n",
        "\r\n",
        "print(\"-- Serial Model --\")\r\n",
        "print(serial,\"\\n\")\r\n",
        "print(\"-- Properties --\")\r\n",
        "print(\"name :\", serial.name)\r\n",
        "print(\"sublayers :\", serial.sublayers)\r\n",
        "print(\"expected inputs :\", serial.n_in)\r\n",
        "print(\"promised outputs :\", serial.n_out)\r\n",
        "print(\"weights & biases:\", serial.weights, \"\\n\")\r\n",
        "\r\n",
        "# Inputs\r\n",
        "print(\"-- Inputs --\")\r\n",
        "print(\"x :\", x, \"\\n\")\r\n",
        "\r\n",
        "# Outputs\r\n",
        "y = serial(x)\r\n",
        "print(\"-- Outputs --\")\r\n",
        "print(\"y :\", y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Serial Model --\n",
            "Serial[\n",
            "  LayerNorm\n",
            "  Relu\n",
            "  TimesTwo\n",
            "] \n",
            "\n",
            "-- Properties --\n",
            "name : Serial\n",
            "sublayers : [LayerNorm, Relu, TimesTwo]\n",
            "expected inputs : 1\n",
            "promised outputs : 1\n",
            "weights & biases: [(DeviceArray([1, 1, 1, 1, 1], dtype=int32), DeviceArray([0, 0, 0, 0, 0], dtype=int32)), (), ()] \n",
            "\n",
            "-- Inputs --\n",
            "x : [-2 -1  0  1  2] \n",
            "\n",
            "-- Outputs --\n",
            "y : [0.        0.        0.        1.4142132 2.8284264]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/_src/numpy/lax_numpy.py:2894: UserWarning: Explicitly requested dtype int64 requested in ones is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"ones\")\n",
            "/usr/local/lib/python3.6/dist-packages/jax/_src/numpy/lax_numpy.py:2885: UserWarning: Explicitly requested dtype int64 requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlmEl-u2cGzS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN5eYxWAcNL_"
      },
      "source": [
        "\r\n",
        "#JAX\r\n",
        "Just remember to lookout for which numpy you are using, the regular ol' numpy or Trax's JAX compatible numpy. Both tend to use the alias np so watch those import blocks.\r\n",
        "\r\n",
        "#####**Note:There are certain things which are still not possible in fastmath.numpy which can be done in numpy so you will see in assignments we will switch between them to get our work done.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w27iufricDdr",
        "outputId": "f68e1762-9d16-43eb-9d9a-858681d3ec95"
      },
      "source": [
        "\r\n",
        "# Numpy vs fastmath.numpy have different data types\r\n",
        "# Regular ol' numpy\r\n",
        "x_numpy = np.array([1, 2, 3])\r\n",
        "print(\"good old numpy : \", type(x_numpy), \"\\n\")\r\n",
        "\r\n",
        "# Fastmath and jax numpy\r\n",
        "x_jax = fastmath.numpy.array([1, 2, 3])\r\n",
        "print(\"jax trax numpy : \", type(x_jax))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good old numpy :  <class 'numpy.ndarray'> \n",
            "\n",
            "jax trax numpy :  <class 'jax.interpreters.xla._DeviceArray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nWvKe5NbFgB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}